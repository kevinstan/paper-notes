# Summary:
In this very recent paper, Yao and Hsu, motivated by human abilities to simulate and imagine the 3D world, propose an approach to incorporate object-based, interpretable scene representations into deep generative models. They make the contribution of using neural networks for both differential encoder and decoder which allow for handling more complex natural images. First, an instance segmentation map is produced while keeping foreground and background separate. Then, color and texture is encoded into latent texture code, whereupon a differential shape renderer can produce the rendered graphics.

# Takeaway:
Combining object-based representations that preserve essential properties such as composi- tionality with powerful deep generative models allows for high-resolution predictions and visualizations of the 3D world.
